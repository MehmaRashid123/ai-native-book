---
sidebar_position: 1
---

# Module 4: Vision-Language-Action (VLA)

In the final module, you'll learn how to create robots that can understand human language, perceive their environment, and take appropriate actions - the essence of embodied AI.

## What You'll Learn

- OpenAI Whisper for voice-to-text processing
- Vision-Language models for perception
- Cognitive planning with Large Language Models (LLMs)
- Closing the loop: Action execution based on understanding
- Capstone: Autonomous Humanoid Project

## Prerequisites

- Understanding of all previous modules
- Basic knowledge of machine learning concepts

## Getting Started

Let's build the complete pipeline from human speech to robot action.